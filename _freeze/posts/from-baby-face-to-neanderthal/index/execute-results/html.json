{
  "hash": "0844c400c53324e7bdc95d9ec316ce1f",
  "result": {
    "markdown": "---\ntitle: \"From Baby Face to Neanderthal\"\ndescription: \"How to turn a series of selfies into a time-lapse video using MediaPipe.\"\nauthor: \"Aswin van Woudenberg\"\ndate: \"2022-07-28\"\ncategories: [programming, python, mediapipe, opencv]\ntoc: true\n---\n\nEarlier this year, I embarked on the famous pilgrimage _Camino de Santiago_, walking from my hometown Leeuwarden in the north of the Netherlands all the way to Santiago de Compostela. For three months, I lived out of a backpack and primarily engaged in wild-camping along the trail. Throughout my journey, I visited some of the most awe-inspiring places, met incredible people along the way, and created unforgettable memories that I will cherish forever.\n\nI didn't want people to think I was just on a huge camping trip to get away from the daily grind, so I told them it was a pilgrimage of \"spiritual growth\" and \"self-discovery\". Some just scoffed, others thought I was losing my mind. \nTo convince my less-enlightened friends that this wasn't just a three-month hike to nowhere, I took a selfie every day to document my, ehm, transcendence.\n\nIn this post I'll show how I turned these daily selfies into a time-lapse video using MediaPipe and OpenCV. \n\n## Import libraries\n\nWe start by importing the necessary libraries. We use MediaPipe to detect face landmarks and OpenCV to scale and translate the images to align them with each other.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport cv2\nimport os\nimport glob\nimport itertools\n\nimport numpy as np\nimport pandas as pd\nimport mediapipe as mp\nimport matplotlib.pyplot as plt\n\nfrom os import listdir\nfrom os.path import isfile, join\n```\n:::\n\n\n## About MediaPipe\n\nMediaPipe is a framework developed by Google that bundles several ML solutions to process images and video. The Face Mesh component estimates 468 3D face landmarks. Below I show how this component can be used. \n\nWe first load an image using the OpenCV 2 library and plot it using Matplotlib.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nFILENAME = \"selfie.jpg\"\nimage = cv2.imread(FILENAME)\n\nplt.figure(figsize=(10, 10))\nplt.title(\"Original\")\nplt.axis('off')\nplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-1.png){width=763 height=597}\n:::\n:::\n\n\nThen we find the landmarks and draw a mesh on the image.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nmp_drawing = mp.solutions.drawing_utils\nmp_drawing_styles = mp.solutions.drawing_styles\nmp_face_mesh = mp.solutions.face_mesh\ndrawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1)\n\nwith mp_face_mesh.FaceMesh(static_image_mode=True, \n            max_num_faces=1, refine_landmarks=True, min_detection_confidence=0.5) as face_mesh:\n    image = cv2.imread(FILENAME)\n    results = face_mesh.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n\n    # draw face mesh landmarks on the image.\n    for face_landmarks in results.multi_face_landmarks:\n        mp_drawing.draw_landmarks(image=image, landmark_list=face_landmarks, \n            connections=mp_face_mesh.FACEMESH_TESSELATION, landmark_drawing_spec=None,\n            connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_tesselation_style())\n        mp_drawing.draw_landmarks(image=image,\n            landmark_list=face_landmarks, connections=mp_face_mesh.FACEMESH_CONTOURS, landmark_drawing_spec=None, \n            connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_contours_style())\n        mp_drawing.draw_landmarks(image=image, landmark_list=face_landmarks,\n            connections=mp_face_mesh.FACEMESH_IRISES, landmark_drawing_spec=None,\n            connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_iris_connections_style())\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nINFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n```\n:::\n:::\n\n\nAnd show the result.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nplt.figure(figsize=(10,10))\nplt.title(\"Result\")\nplt.axis('off')\nplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-1.png){width=763 height=597}\n:::\n:::\n\n\n## Selecting landmarks\n\nWe're only interested in a few specific landmarks to figure out how to scale and translate images so that they can be stitched together into a time-lapse video. More specifically, we are interested in the landmarks corresponding to the nose and the left- and rightmost points of the face. The landmark that corresponds with the tip of the nose is used to align images. The left- and rightmost landmarks are used to determine how much to scale them.\n\nThe index numbers for the different landmarks can be found in [this image](https://github.com/google/mediapipe/blob/a908d668c730da128dfa8d9f6bd25d519d006692/mediapipe/modules/face_geometry/data/canonical_face_model_uv_visualization.png?raw=true).\n\nWe define some constants including the indexes of the landmarks we're interested in. You can experiment with the FPS parameter that tells you how many different frames will be shown per second.\n\n```{.python}\nIMG_PATH = \"selfies/\" # the input directory\nOUTPUT_PATH = \"output/\" # also used for temporary files\nVIDEO_NAME = 'video.avi' # the output filename\nFPS = 3 # frames per second\n\nNOSE_TIP_LANDMARK = 1\nLEFTMOST_LANDMARK = 234\nRIGHTMOST_LANDMARK = 454\n```\n\n## Clearing old files\n\nTo start, we'll delete all files in `OUTPUT_PATH` that might still be there from previous runs.\n\n```{.python}\nfiles = glob.glob(os.path.join(OUTPUT_PATH, \"*\"))\nfor f in files:\n    os.remove(f)\n```\n\n## Defining helper functions\n\nWe need to define a couple of helper functions. \n\nMediapipe landmarks are defined as 3D coordinates. The following function converts a landmark into a 2D pixel coordinate.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\ndef to_pixel_coord(image, landmark):\n    # convert landmark to pixel coordinates\n    [height, width, _] = image.shape\n    return int(landmark.x * width), int(landmark.y * height)\n```\n:::\n\n\nAnother function loops through all files in a directory and tries to determine the landmarks' pixel coordinates. The result is returned as a Pandas dataframe.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\ndef read_landmarks(path):\n    # find all files in directory\n    filenames = [f for f in listdir(path) if isfile(join(path, f))]\n    filenames.sort()\n    \n    # create an empty dataframe\n    columns = {\n        \"file\": str(), \n        \"nose_tip_x\": int(), \"nose_tip_y\": int(), \n        \"leftmost_x\": int(), \"leftmost_y\": int(), \n        \"rightmost_x\": int(), \"rightmost_y\": int(),\n        \"width\": int(), \"height\": int()\n    }\n    df = pd.DataFrame(columns, index=[])\n    \n    # find the landmarks' pixel coordinates\n    with mp_face_mesh.FaceMesh(static_image_mode=True, \n                max_num_faces=1, refine_landmarks=True, \n                min_detection_confidence=0.5) as face_mesh:\n        for file in filenames:\n            image = cv2.imread(os.path.join(path, file))\n            results = face_mesh.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n\n            if not len(results.multi_face_landmarks) == 1:\n                # detected less or more than one face -> skip image\n                continue\n            face_landmarks = results.multi_face_landmarks[0]\n            nose_tip_x, nose_tip_y = to_pixel_coord(image, face_landmarks.landmark[NOSE_TIP_LANDMARK])\n            leftmost_x, leftmost_y = to_pixel_coord(image, face_landmarks.landmark[LEFTMOST_LANDMARK])\n            rightmost_x, rightmost_y = to_pixel_coord(image, face_landmarks.landmark[RIGHTMOST_LANDMARK])\n            [height, width, _] = image.shape\n            landmarks_xy = [file, nose_tip_x, nose_tip_y, leftmost_x, leftmost_y, rightmost_x, rightmost_y, width, height]\n            df = pd.concat([df, pd.DataFrame([landmarks_xy], columns=list(columns.keys()))], ignore_index=True)\n    \n    return df\n```\n:::\n\n\nWe also need a function to scale images.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\ndef scale_image(filename_input, filename_output, factor):\n    # read image from disk\n    image = cv2.imread(filename_input)\n    \n    (height, width) = image.shape[:2]\n\n    res = cv2.resize(image, (int(width * factor), int(height * factor)), interpolation=cv2.INTER_CUBIC)\n  \n    # write image back to disk.\n    cv2.imwrite(filename_output, res)\n```\n:::\n\n\nThe next function translates an image. Translating an image means shifting it within a given frame of reference.\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\ndef translate_image(filename_input, filename_output, x, y):\n    # if the shift is (x, y) then the translation matrix would be\n    # M = [1 0 x]\n    #     [0 1 y]\n    M = np.float32([[1, 0, x], [0, 1, y]])\n    \n    # read image from disk.\n    image = cv2.imread(filename_input)\n    (rows, cols) = image.shape[:2]\n  \n    # warpAffine does appropriate shifting given the translation matrix.\n    res = cv2.warpAffine(image, M, (cols, rows))\n  \n    # write image back to disk.\n    cv2.imwrite(filename_output, res)\n```\n:::\n\n\n## Processing the images\n\nNow we can process our collection of selfies. This means finding landmarks, scaling, and translating images so that they align properly.\n\n### Finding landmarks\n\nWe can just call the function we define before.\n\n```{.python}\ndf = read_landmarks(IMG_PATH)\n```\n\n### Scaling images\n\nBy rescaling the images we make the face in each photo of similar size. \n\nWe first determine the mean size of the face.\n\n```{.python}\nmean_face_size = int(df.rightmost_x.mean()) - int(df.leftmost_x.mean())\n```\n\nAfter finding the mean face size, we rescale each image to match this.\n\n```{.python}\nfor _, row in df.iterrows():\n    filename = row['file']\n    face_size = row['rightmost_x'] - row['leftmost_x']\n    scale_image(os.path.join(IMG_PATH, filename), os.path.join(OUTPUT_PATH, filename), mean_face_size / face_size)\n```\n\nAfter rescaling, we need to find the landmarks again as they have changed.\n\n```{.python}\ndf = read_landmarks(OUTPUT_PATH)\n```\n\n### Translating images\n\nIn this step we find the average location of the tip of the nose. Then we translate all images so that the tips of the noses align. We keep track of how much an image is maximally shifted in the horizontal or vertical direction so we can properly crop the images later.\n\n```{.python}\nmean_x = int(df.nose_tip_x.mean())\nmean_y = int(df.nose_tip_y.mean())\n```\n\n```{.python}\ncrop_left = 0\ncrop_right = 0\ncrop_top = 0\ncrop_bottom = 0\n```\n\n```{.python}\nfor _, row in df.iterrows():\n    filename = row['file']\n    shift_x = mean_x - row['nose_tip_x']\n    shift_y = mean_y - row['nose_tip_y']\n    translate_image(os.path.join(OUTPUT_PATH, filename), os.path.join(OUTPUT_PATH, filename), shift_x, shift_y)\n    \n    if shift_x > 0 and shift_x > crop_left:\n        crop_left = shift_x\n    elif shift_x < 0 and abs(shift_x) > crop_right:\n        crop_right = abs(shift_x)\n    elif shift_y > 0 and shift_y > crop_top:\n        crop_top = shift_y\n    elif shift_y < 0 and abs(shift_y) > crop_bottom:\n        crop_bottom = abs(shift_y)\n```\n\n### Cropping images\n\nBecause of translating we ended up with images with black bars on the sides. We crop images so that these disappear. This way we end up with images that are a bit smaller.\n\n```{.python}\nmin_width = df.width.min()\nmin_height = df.height.min()\n\nfor _, row in df.iterrows():\n    filename = row['file']\n    image = cv2.imread(os.path.join(OUTPUT_PATH, filename))\n    (rows, cols) = image.shape[:2]\n    res = image[crop_top:min_height, crop_left:min_width]\n    cv2.imwrite(os.path.join(OUTPUT_PATH, filename), res)\n```\n\n# Creating the final video\n\nWe can now concatenate all images into a video and delete any temporary files.\n\n```{.python}\nimages = [img for img in os.listdir(OUTPUT_PATH)]\nimages.sort()\nframe = cv2.imread(os.path.join(OUTPUT_PATH, images[0]))\nheight, width, layers = frame.shape\n\nvideo = cv2.VideoWriter(os.path.join(OUTPUT_PATH, VIDEO_NAME), 0, FPS, (width, height))\n\nfor image in images:\n    video.write(cv2.imread(os.path.join(OUTPUT_PATH, image)))\n\ncv2.destroyAllWindows()\nvideo.release()\n\nfor image in images:\n    os.remove(os.path.join(OUTPUT_PATH, image))\n```\n\nThe final result looks like this:\n\n<iframe width=\"461\" height=\"820\" src=\"https://www.youtube.com/embed/KTt9A1DDsQU\" title=\"From Baby Face to Neanderthal\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>\n\nLet me know in the comments what you think. Isn't this time-lapse of my baby face turned Neanderthal clear evidence of my \"spiritual ascension\"?\n\n# Creating your own time-lapse\n\nIf you want to create your own time-lapse video, check out one of the following links:\n\n<a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://gist.githubusercontent.com/afvanwoudenberg/601846fbdd0ea04324769885b679079a/raw/18f0d454c0ce855ef8ee8f009a1e1a2f1994c95f/time-lapse.ipynb\">![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)</a> \n<a target=\"_blank\" href=\"http://colab.research.google.com/gist/afvanwoudenberg/601846fbdd0ea04324769885b679079a/time-lapse.ipynb\">![Colab](https://colab.research.google.com/assets/colab-badge.svg)</a>\n<a target=\"_blank\" href=\"https://gist.github.com/afvanwoudenberg/601846fbdd0ea04324769885b679079a\">![GitHub](https://shields.io/badge/-View%20as%20Gist-grey.svg?logo=github&style=flat&logoColor=white&labelColor=black)</a>\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}