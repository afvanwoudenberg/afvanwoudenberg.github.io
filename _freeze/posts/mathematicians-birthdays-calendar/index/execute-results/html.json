{
  "hash": "02e453bd89819b0cd3c137c3d8c6d86c",
  "result": {
    "markdown": "---\ntitle: \"Mathematicians' Birthdays Calendar\"\ndescription: \"Web scraping Wikipedia to create a mathematicians' birthdays calendar.\"\nauthor: \"Aswin van Woudenberg\"\ndate: \"2023-12-25\"\ncategories: [programming, python]\ndraft: true\ntoc: true\n---\n\nI teach several courses in the Applied Mathematics program at NHL Stenden University of Applied Sciences. My favorite days are the ones when there's something to celebrate (like a birthday) and someone brings cake.\n\nIn an effort to convince my department that we should have cake more often, I've created a birthday calendar featuring the birthdates of mathematicians, both living and deceased. I scraped these birthdays from Wikipedia and converted them into an ICS file that can be imported into a calendar application like Google Calendar or Microsoft Outlook Calendar.\n\nRead on to learn how I did this.\n\n## Importing libraries\n\nWe start by importing some libraries. I'm using [Selenium](https://www.selenium.dev/) for webscraping. The [icalendar](https://pypi.org/project/icalendar/) package is used to create the birthday calendar.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nfrom selenium import webdriver\nfrom selenium.webdriver.chrome.service import Service\nfrom selenium.webdriver.common.by import By\nfrom selenium.common.exceptions import NoSuchElementException\nfrom webdriver_manager.chrome import ChromeDriverManager\n\nfrom icalendar import Calendar, Event, vRecur\nfrom datetime import date\n\nfrom abc import ABC, abstractmethod\n\nimport pandas as pd\n\nimport re\nimport calendar\nimport pickle\nimport json\n```\n:::\n\n\n## Scraping Wikipedia\n\nWikipedia has, for each day of the year, an entry listing historical events, births, and deaths. We will scrape, for each day in a year, from this list, the births of mathematicians and place them in a dictionary. \n\nTo see what I mean, let's look at the entry for [December, the 7th](https://en.wikipedia.org/wiki/December_7).\n\n![](december_7.png)\n\nEach wikipedia page has a title which is shown here with a red border. If we scroll down we see a list of people who were born on this day. The first mathematician we find is [Leopold Kronecker](https://en.wikipedia.org/wiki/Leopold_Kronecker).\n\n![](leopold_kronecker_december_7.png)\n\nScrolling further down, we find three more mathematicians. \n\nThe idea is to open for each day in the year its corresponding Wikipedia page. Using [XPATH](https://en.wikipedia.org/wiki/XPath) expressions, we'll extract the title of the page and each row in the list of births that contains the word _mathematician_. Each row, contains the year the mathematician was born (_1823_), their name (_Leopold Kronecker_), a one-line bio (_Polish-German mathematician and academic (d. 1891)_), and a URL pointing to a Wikipedia page about the mathematician in question (_https://en.wikipedia.org/wiki/Leopold_Kronecker_).\n\n## Building the scraper\n\nThere are various ways to scrape using Python. Popular scraping libraries include [Scrapy](https://scrapy.org/) and [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/). I usually use Selenium because it allows me to scrape from websites that render HTML from JavaScript. Selenium can be a bit slow, but we're only scraping 366 pages so it won't take that long. \n\nIn previous projects I have created some helper classes that allow me to quickly set up a scraping task. These classes can be easily chained together to define a new scraper. I got inspired by scraper tools such as [Web scraper](https://webscraper.io/) and [ParseHub](https://www.parsehub.com/) that let you compose a tree that that specifies how to parse data from a website. Using the classes I defined, we can do something similar in Python.\n\nAll these helper classes inherit from an [abstract base class](https://en.wikipedia.org/wiki/Class_(computer_programming)#Abstract_and_concrete) called `BaseSelector`.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nclass BaseSelector(ABC):\n    @abstractmethod\n    def scrape(self, scraper, parent, **kwargs):\n        pass\n```\n:::\n\n\nClasses that inherit from `BaseSelector` need to provide an implementation of the `scrape` method. This method generally scrapes some content from a webpage and returns this wrapped up in a Python dictionary. It may scrape a single DOM element, and return a single dictionary object, or it may scrape multiple elements and return these as a list of dictionaries. \n\n### Scraping attributes\n\nThe following class is one of these classes that inherit from `BaseSelector`. It locates an element(s) and returns a specific attribute of that element, such as the _href_ (in case it matches an anchor link).\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nclass AttributeSelector(BaseSelector):\n    def __init__(self, id, by, path, attribute, multiple=False, default=None):\n        self.id = id\n        self.by = by\n        self.path = path\n        self.attribute = attribute\n        self.multiple = multiple\n        self.default = default\n\n    def scrape(self, scraper, parent, **kwargs):    \n        if not self.multiple:\n            try:\n                elem = parent.find_element(self.by, self.path.format(**kwargs))\n                return {self.id: elem.get_attribute(self.attribute)}\n            except NoSuchElementException:\n                return self.default\n        else:\n            elems = parent.find_elements(self.by, self.path.format(**kwargs))\n            return [{self.id: elem.get_attribute(self.attribute)} for elem in elems]\n```\n:::\n\n\nLet's see how we can use it to scrape the URLs for each mathematicians found on a page (e.g. [https://en.m.wikipedia.org/wiki/Leopold_Kronecker](https://en.m.wikipedia.org/wiki/Leopold_Kronecker)).\n\nWe create a new `AttributeSelector` as follows.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nurls_selector = AttributeSelector(\n    'url', \n    By.XPATH, \n    \"//*[@id='mw-content-text']/div/ul[preceding::h2/span[.='Births'] and following::h2/span[.='Deaths']]/li[contains(., 'mathematician')]/a[string-length(.)>4][1]\", \n    'href', \n    multiple=True\n)\n```\n:::\n\n\nWe provide a key (_'url'_) for the dictionary object. \n\nWe also tell it to search for an element addressed by an XPATH expression. In this case we look for list items that contain the string _mathematician_ and find within each list item the first link with a content length of greater than four. We check for length because sometimes it may find multiple links on a single line but the first link always refers to a _year_ (and is therefor at most 4 digits long). Note how we're only looking for `li` elements after a `span` that contains the text _Births_, but before a `span` that contains the word _Deaths_. XPATH expressions can get a bit unwieldly but they're very powerful.\n\nLinks (`a` or _anchor_ tags) have an `href` attribute that we are interested in. We can expect multiple results as there may be multiple mathematicians born on the same day.\n\nBefore we can call this `urls_selector` object, we first need to initilize a Selenium webdriver.\n\n```{.python}\ndriver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n```\n\nThis will open a new instance of Google Chrome. Now we can open the webpage we want to scrape from.\n\n```{.python}\ndriver.get(\"https://en.wikipedia.org/wiki/December_7\")\n```\n\nAnd we get the URLs we're interested in.\n\n```{.python}\nurls = urls_selector.scrape(driver, driver)\n```\n\n\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nurls\n```\n\n::: {.cell-output .cell-output-display execution_count=142}\n```\n[{'url': 'https://en.wikipedia.org/wiki/Leopold_Kronecker'},\n {'url': 'https://en.wikipedia.org/wiki/Danilo_BlanuC5%A1a'},\n {'url': 'https://en.wikipedia.org/wiki/Mary_Ellen_Rudin'},\n {'url': 'https://en.wikipedia.org/wiki/Nick_Katz'}]\n```\n:::\n:::\n\n\n### Scraping text\n\nMost often we are interested in the text that is contained in an element. To scrape simple text we can use the `TextSelector` class.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nclass TextSelector(AttributeSelector):\n    def __init__(self, id, by, path, multiple=False, default=None):\n        super().__init__(id, by, path, 'innerText', multiple, default)\n```\n:::\n\n\nIt extends the AttributeSelector class we defined before. We can use it as follows to get all text of lines that contain the word _mathematician_.\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nmathematician_selector = TextSelector(\n    'mathematician', \n    By.XPATH, \n    \"//*[@id='mw-content-text']/div/ul[preceding::h2/span[.='Births'] and following::h2/span[.='Deaths']]/li[contains(., 'mathematician')]\", \n    multiple=True\n)\n```\n:::\n\n\nAgain we use XPATH to address the elements we're interested in. This time we don't have to check for URL length, so the expression is a bit simpler. Let's use it to get each line corresponding to the birth of a mathematician.\n\n```{.python}\nmathematicians = mathematician_selector.scrape(driver, driver)\n```\n\n\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nmathematicians\n```\n\n::: {.cell-output .cell-output-display execution_count=146}\n```\n[{'mathematician': '1823 – Leopold Kronecker, Polish-German mathematician and academic (d. 1891)'},\n {'mathematician': '1903 – Danilo Blanuša, Croatian mathematician, physicist, and academic (d. 1987)'},\n {'mathematician': '1924 – Mary Ellen Rudin, American mathematician (d. 2013)[16]'},\n {'mathematician': '1943 – Nick Katz, American mathematician and academic'}]\n```\n:::\n:::\n\n\nSimilarly we can scrape the month and day from the top of the page.\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nmonth_day_selector = TextSelector('month_day', By.XPATH, '//*[@id=\"firstHeading\"]/span')\n```\n:::\n\n\n```{.python}\nmonth_day = month_day_selector.scrape(driver, driver)\n```\n\n\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\nmonth_day\n```\n\n::: {.cell-output .cell-output-display execution_count=149}\n```\n{'month_day': 'December 7'}\n```\n:::\n:::\n\n\n### Cleaning and transforming data\n\nWe see that the third item in the `mathematicians` list, contains a reference as indicated by `[16]`. We want to get rid of this. Furthermore, we also want to split the values for the key `mathematician` into several keys, such as, `year`, `name`, `bio`.\n\nThe value `month_day` contains both the month and day. This is another thing we will want to split up into two seperate keys.\n\nWe could leave it as is, and do some data cleaning after having scraped all pages. With the approach I'm taking here, it's however quite easy to immediately clean and transform the data _while_ scraping. \n\nI defined a `MapSelector` class that let's me map a function over scraped data.\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\nclass MapSelector(BaseSelector):\n    def __init__(self, fun, child):\n        self.fun = fun\n        self.child = child\n\n    def scrape(self, scraper, parent, **kwargs):\n        data = self.child.scrape(scraper, parent, **kwargs)\n        if type(data) == dict:\n            return self.fun(data)\n        return list(map(self.fun, data))\n```\n:::\n\n\nThis class doesn't do any actual scraping, but is meant to be chained with other _selector_ classes. Let's see how it can be used to split up `month_day` values. We'll also convert the month name to its number.\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\nsplit_month_day_selector = MapSelector(\n    lambda m: { \n        \"day\": int(m['month_day'].split()[1]), \n        \"month\": list(calendar.month_name).index(m['month_day'].split()[0]) \n    },\n    month_day_selector\n)\n```\n:::\n\n\nNote how I'm specifying a lambda function to split up `month_day` into two seperate keys. I'm passing the `month_day_selector` we defined above to the constructor as well so the `MapSelector` knows where to get data from before applying the function.\n\n```{.python}\nsplit_month_day = split_month_day_selector.scrape(driver, driver)\n```\n\nCalling `scrape` on the `split_month_day_selector` object will cause it to call the `scrape` function on the `month_day_selector` which does the actual scraping. \n\n\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\nsplit_month_day\n```\n\n::: {.cell-output .cell-output-display execution_count=153}\n```\n{'day': 7, 'month': 12}\n```\n:::\n:::\n\n\nInstead of providing a `lambda` function, we can use a regular function. Let's create a function that cleans and splits up the `mathematician` key in the `mathematicians` list.\n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\ndef unpack_mathematician_data(m):\n    data = re.sub(r'\\[[^\\]]*\\]', '', m['mathematician']) # remove any references\n    match = re.match(r'^(\\d+)\\s.*–\\s*(.*?),\\s*(.*?)$', data)\n\n    return {\n        \"year\": int(match.group(1)),\n        \"name\": match.group(2),\n        \"bio\": match.group(3)\n    }\n```\n:::\n\n\n::: {.cell execution_count=19}\n``` {.python .cell-code}\nsplit_mathematicians_selector = MapSelector(unpack_mathematician_data, mathematician_selector)\n```\n:::\n\n\n```{.python}\nsplit_mathematicians = split_mathematicians_selector.scrape(driver, driver)\n```\n\nNow we have our data in seperate keys. \n\n\n\n::: {.cell execution_count=21}\n``` {.python .cell-code}\nsplit_mathematicians\n```\n\n::: {.cell-output .cell-output-display execution_count=157}\n```\n[{'year': 1823,\n  'name': 'Leopold Kronecker',\n  'bio': 'Polish-German mathematician and academic (d. 1891)'},\n {'year': 1903,\n  'name': 'Danilo Blanuša',\n  'bio': 'Croatian mathematician, physicist, and academic (d. 1987)'},\n {'year': 1924,\n  'name': 'Mary Ellen Rudin',\n  'bio': 'American mathematician (d. 2013)'},\n {'year': 1943,\n  'name': 'Nick Katz',\n  'bio': 'American mathematician and academic'}]\n```\n:::\n:::\n\n\n### Merging data\n\nWe now have _selectors_ that we can call to scrape a list of dictionaries with the _year_, _name_, and _biography_ of each mathematician, and a dictionary with the _month_ and _day_ these mathematicians were born. We also defined a _selector_ that scrapes a list of dictionaries of _URLs_.\n\nIn order to merge data from different _selectors_ I defined a `ZipSelector`. If it merges two lists, the dicts in the first list will be extended with the keys and values of the dicts in the second list. It will only return as many items as are contained in the shortest list (similar to Python `zip`). If a list is merged with a dictionary, the dictionary keys are added to each dictionary in the list. If it merges two dictionaries, it returns a single dictionary containing the keys and values of both dictionaries.\n\n::: {.cell execution_count=22}\n``` {.python .cell-code}\nclass ZipSelector(BaseSelector):\n    def __init__(self, left, right):\n        self.left = left\n        self.right = right\n        \n    def scrape(self, scraper, parent, **kwargs):\n        data_left = self.left.scrape(scraper, parent, **kwargs)\n        data_right = self.right.scrape(scraper, parent, **kwargs)\n        \n        if type(data_left) == dict and type(data_right) == dict:\n            return data_left | data_right\n        elif type(data_left) == dict and type(data_right) == list:\n            return [data_left | m for m in data_right]\n        elif type(data_left) == list and type(data_right) == dict:\n            return [m | data_right for m in data_left]\n        return [data_left[i] | data_right[i] for i in range(min(len(data_left),len(data_right)))]\n```\n:::\n\n\nLet's see how it works.\n\n::: {.cell execution_count=23}\n``` {.python .cell-code}\nzip_month_day_mathematicians_selector = ZipSelector(split_month_day_selector, split_mathematicians_selector)\nzip_month_day_mathematicians_urls_selector = ZipSelector(zip_month_day_mathematicians_selector, urls_selector)\n```\n:::\n\n\nWe first merge the _month_ and _day_ with the _year_, _name_, and _bio_. Then using a second `ZipSelector` we combine this with _url_.\n\n```{.python}\nzip_month_day_mathematicians_urls = zip_month_day_mathematicians_urls_selector.scrape(driver, driver)\n```\n\nIf it wasn't clear already: we don't need to call the `scrape` method on each _selector_ object seperately. We are combining _selector_ objects into a tree and just call `scrape` on the root node and this causes a cascade of calls throughout the tree.\n\n\n\n::: {.cell execution_count=25}\n``` {.python .cell-code}\nzip_month_day_mathematicians_urls\n```\n\n::: {.cell-output .cell-output-display execution_count=161}\n```\n[{'day': 7,\n  'month': 12,\n  'year': 1823,\n  'name': 'Leopold Kronecker',\n  'bio': 'Polish-German mathematician and academic (d. 1891)',\n  'url': 'https://en.wikipedia.org/wiki/Leopold_Kronecker'},\n {'day': 7,\n  'month': 12,\n  'year': 1903,\n  'name': 'Danilo Blanuša',\n  'bio': 'Croatian mathematician, physicist, and academic (d. 1987)',\n  'url': 'https://en.wikipedia.org/wiki/Danilo_Blanu%C5%A1a'},\n {'day': 7,\n  'month': 12,\n  'year': 1924,\n  'name': 'Mary Ellen Rudin',\n  'bio': 'American mathematician (d. 2013)',\n  'url': 'https://en.wikipedia.org/wiki/Mary_Ellen_Rudin'},\n {'day': 7,\n  'month': 12,\n  'year': 1943,\n  'name': 'Nick Katz',\n  'bio': 'American mathematician and academic',\n  'url': 'https://en.wikipedia.org/wiki/Nick_Katz'}]\n```\n:::\n:::\n\n\n### Navigating pages\n\nWe can now call the `zip_month_day_mathematicians_urls_selector` for each page we want to scrape. This means calling the `get` method on the `driver` object to navigate to each page and then repeatedly call the `scrape` method on the `zip_month_day_mathematicians_urls_selector` object and concatenate all the results. To do that automatically I've created, you guessed it, another class. The `URLSelector` class takes a list of URLs and opens them one by one each time calling a _selector_ instance to scrape the data. All the data that is scraped is concatenated and eventually returned.\n\n::: {.cell execution_count=26}\n``` {.python .cell-code}\nclass URLSelector(BaseSelector):\n    def __init__(self, urls, child):\n        self.urls = urls\n        self.child = child\n        \n    def scrape(self, scraper, parent, **kwargs):\n        result = []\n        \n        for url in self.urls:\n            scraper.get(url.format(**kwargs))\n            data = self.child.scrape(scraper, scraper, **kwargs)\n            \n            if type(data) == dict:\n                result.append(data)\n            elif type(data) == list:\n                result.extend(data)\n            \n        return result\n```\n:::\n\n\nThe constructor takes a list of URLs and a _selector_ class to call for each page. \n\nWe start by creating a list of all URLs we will scrape from.\n\n::: {.cell execution_count=27}\n``` {.python .cell-code}\nmonth_names = calendar.month_name[1:]\ndays_in_month = [calendar.monthrange(2020, month)[1] for month in range(1, 13)]\nmonths_and_days = list(zip(month_names, days_in_month))\n\nwikipedia_urls = [f\"https://en.wikipedia.org/wiki/{month}_{day+1}\" for (month, days) in months_and_days for day in range(days)]\n```\n:::\n\n\nNote how I selected a leap year (2020) to make sure I get 366 URLs. Let's show the first five.\n\n::: {.cell execution_count=28}\n``` {.python .cell-code}\nwikipedia_urls[:5]\n```\n\n::: {.cell-output .cell-output-display execution_count=164}\n```\n['https://en.wikipedia.org/wiki/January_1',\n 'https://en.wikipedia.org/wiki/January_2',\n 'https://en.wikipedia.org/wiki/January_3',\n 'https://en.wikipedia.org/wiki/January_4',\n 'https://en.wikipedia.org/wiki/January_5']\n```\n:::\n:::\n\n\nNow we create an instance of `URLSelector` and pass it this list of URLs and the _selector_ object to call for each page, in this case `zip_month_day_mathematicians_urls_selector`.\n\n::: {.cell execution_count=29}\n``` {.python .cell-code}\nscraper = URLSelector(wikipedia_urls, zip_month_day_mathematicians_urls_selector)\n```\n:::\n\n\nWe build the scraper step by step. Here's an overview of how we combined the various classes.\n\n\n```{dot}\nstrict digraph scraper { \n    graph [rankdir=LR, ratio=0.4]\n    edge[color=black]\n    node[width=2, shape=rectangle, style=filled, colorscheme=ylgnbu3]\n    us [label=\"URLSelector\", color=2]\n    zs1 [label=\"ZipSelector\", color=lightblue]\n    zs2 [label=\"ZipSelector\", color=lightblue]\n    ms1 [label=\"MapSelector\", color=lightgrey]\n    ts1 [label=\"TextSelector\", color=lightgreen]\n    ms2 [label=\"MapSelector\", color=lightgrey]\n    ts2 [label=\"TextSelector\", color=lightgreen]\n    as [label=\"AttributeSelector\", color=lightgreen]\n    us -> zs1\n    zs1 -> zs2\n    zs1 -> as\n    zs2 -> ms1\n    zs2 -> ms2\n    ms1 -> ts1\n    ms2 -> ts2\n}\n```\n\n\nThe direction of the arrows indicate the direction of the cascade of calls. The data flows in the opposite direction.\n\nOnly `TextSelector` and `AttributeSelector` do any actual scraping. `URLSelector` navigates to each URL. The other _selector_ classes merge or transform the data. In other scraping projects I might use custom _selector_ classes that take care of pagination or filling forms and pressing buttons. \n\n## Running the scraper\n\nCalling the `scrape` method will scrape all mathematicians' birthdays.\n\n```{.python}\ndata = scraper.scrape(driver, driver)\n```\n\n\n\nLet's looks at the first five results.\n\n::: {.cell execution_count=31}\n``` {.python .cell-code}\ndata[:5]\n```\n\n::: {.cell-output .cell-output-display execution_count=167}\n```\n[{'day': 1,\n  'month': 1,\n  'year': 1878,\n  'name': 'Agner Krarup Erlang',\n  'bio': 'Danish mathematician, statistician, and engineer (d. 1929)',\n  'url': 'https://en.wikipedia.org/wiki/Agner_Krarup_Erlang'},\n {'day': 1,\n  'month': 1,\n  'year': 1894,\n  'name': 'Satyendra Nath Bose',\n  'bio': 'Indian physicist and mathematician (d. 1974)',\n  'url': 'https://en.wikipedia.org/wiki/Satyendra_Nath_Bose'},\n {'day': 1,\n  'month': 1,\n  'year': 1905,\n  'name': 'Stanisław Mazur',\n  'bio': 'Ukrainian-Polish mathematician and theorist (d. 1981)',\n  'url': 'https://en.wikipedia.org/wiki/Stanis%C5%82aw_Mazur'},\n {'day': 1,\n  'month': 1,\n  'year': 1912,\n  'name': 'Boris Vladimirovich Gnedenko',\n  'bio': 'Russian mathematician and historian (d. 1995)',\n  'url': 'https://en.wikipedia.org/wiki/Boris_Vladimirovich_Gnedenko'},\n {'day': 2,\n  'month': 1,\n  'year': 1803,\n  'name': 'Guglielmo Libri Carucci dalla Sommaja',\n  'bio': 'Italian mathematician and academic (d. 1869)',\n  'url': 'https://en.wikipedia.org/wiki/Guglielmo_Libri_Carucci_dalla_Sommaja'}]\n```\n:::\n:::\n\n\nWe can save the result as a JSON file.\n\n```{.python}\n# Save to a JSON file\nwith open('data.json', 'w') as json_file:\n    json.dump(data, json_file)\n```\n\nOr convert it to a dataframe and save it as CSV.\n\n::: {.cell .column-screen-inset execution_count=32}\n``` {.python .cell-code}\ndf = pd.DataFrame(data)\ndf\n```\n\n::: {.cell-output .cell-output-display execution_count=168}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>day</th>\n      <th>month</th>\n      <th>year</th>\n      <th>name</th>\n      <th>bio</th>\n      <th>url</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1878</td>\n      <td>Agner Krarup Erlang</td>\n      <td>Danish mathematician, statistician, and engine...</td>\n      <td>https://en.wikipedia.org/wiki/Agner_Krarup_Erlang</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1894</td>\n      <td>Satyendra Nath Bose</td>\n      <td>Indian physicist and mathematician (d. 1974)</td>\n      <td>https://en.wikipedia.org/wiki/Satyendra_Nath_Bose</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1905</td>\n      <td>Stanisław Mazur</td>\n      <td>Ukrainian-Polish mathematician and theorist (d...</td>\n      <td>https://en.wikipedia.org/wiki/Stanis%C5%82aw_M...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1912</td>\n      <td>Boris Vladimirovich Gnedenko</td>\n      <td>Russian mathematician and historian (d. 1995)</td>\n      <td>https://en.wikipedia.org/wiki/Boris_Vladimirov...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2</td>\n      <td>1</td>\n      <td>1803</td>\n      <td>Guglielmo Libri Carucci dalla Sommaja</td>\n      <td>Italian mathematician and academic (d. 1869)</td>\n      <td>https://en.wikipedia.org/wiki/Guglielmo_Libri_...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>833</th>\n      <td>28</td>\n      <td>12</td>\n      <td>1950</td>\n      <td>Clifford Cocks</td>\n      <td>English mathematician and cryptographer</td>\n      <td>https://en.wikipedia.org/wiki/Clifford_Cocks</td>\n    </tr>\n    <tr>\n      <th>834</th>\n      <td>29</td>\n      <td>12</td>\n      <td>1856</td>\n      <td>Thomas Joannes Stieltjes</td>\n      <td>Dutch-French mathematician and academic (d. 1894)</td>\n      <td>https://en.wikipedia.org/wiki/Thomas_Joannes_S...</td>\n    </tr>\n    <tr>\n      <th>835</th>\n      <td>30</td>\n      <td>12</td>\n      <td>1944</td>\n      <td>Joseph Hilbe</td>\n      <td>American mathematician and philosopher (d. 2017)</td>\n      <td>https://en.wikipedia.org/wiki/Joseph_Hilbe</td>\n    </tr>\n    <tr>\n      <th>836</th>\n      <td>31</td>\n      <td>12</td>\n      <td>1714</td>\n      <td>Arima Yoriyuki</td>\n      <td>Japanese mathematician and educator (d. 1783)</td>\n      <td>https://en.wikipedia.org/wiki/Arima_Yoriyuki</td>\n    </tr>\n    <tr>\n      <th>837</th>\n      <td>31</td>\n      <td>12</td>\n      <td>1952</td>\n      <td>Vaughan Jones</td>\n      <td>New Zealand mathematician and academic (d. 2020)</td>\n      <td>https://en.wikipedia.org/wiki/Vaughan_Jones</td>\n    </tr>\n  </tbody>\n</table>\n<p>838 rows × 6 columns</p>\n</div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=33}\n``` {.python .cell-code}\ndf.to_csv('data.csv', index=False)\n```\n:::\n\n\n## Creating the calendar\n\nFinally, we convert this data to an ICS calendar file that can be imported into most calendar applications, like Google Calendar or Microsoft Outlook.\n\n::: {.cell execution_count=34}\n``` {.python .cell-code}\n# Create the calendar\ncal = Calendar()\n\n# Iterate through the birthday data and add recurring events to the calendar\nfor person in data:\n    name = person['name']\n    birthdate = date(person['year'], person['month'], person['day'])\n    bio = person['bio']\n    url = person['url']\n\n    # Create an event for the birthday with recurrence rule\n    event = Event()\n    event.add('summary', f\"{name}'s birthday\")\n    event.add('description', f\"{name} - {bio}\\n\\n{url}\")\n    event.add('dtstart', birthdate)\n    event.add('rrule', {'freq': 'yearly'})\n    event.add('url', url)\n    event.add('transp', 'TRANSPARENT') # Make events not show up as 'busy'\n\n    cal.add_component(event)\n\n# Save the calendar to a file\nwith open('mathematicians_birthdays_calendar.ics', 'wb') as ics_file:\n    ics_file.write(cal.to_ical())\n```\n:::\n\n\nThe file is saved as [mathematicians_birthdays_calendar.ics](mathematicians_birthdays_calendar.ics) and ready to be imported into your calendar.\n\n## Importing the calendar\n\n\n\n## Isaac Newton's two birthdays\n\nYou may have noticed, after viewing this calendar, that Isaac Newton seemingly has two birthdays, ten days apart. Initially recognized as born on December 25, 1642, his birthday is now more commonly acknowledged as January 4, 1643. This is due to the calendar difference between England and the rest of Europe during his birth. While England stuck to the Julian calendar, lagging ten days behind, the continent had already adopted the Gregorian calendar. \n\nWhile January 4 is now widely acknowledged as Isaac Newton's birthday, December 25 continues to be celebrated as [Grav-Mass day](https://www.timeanddate.com/holidays/fun/grav-mass-day). This unofficial holiday annually honors Newton's contributions to the understanding of gravity and mass in physics. \n\nI left both days in the calendar because it means I get to eat even more cake!\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}